{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "236fe6aa-a953-40d2-830b-d6a8db58d7b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unable to convert function return value to a Python type! The signature was\n\t() -> handle",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21336\\668763655.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmake_classification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_typing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmonitoring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tfe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_conversion_registry\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcpp_shape_inference_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0m_np_bfloat16\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpywrap_bfloat16\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbfloat16_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[0m_np_float8_e4m3fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_pywrap_float8\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_float8_e4m3fn_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[0m_np_float8_e5m2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_pywrap_float8\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_float8_e5m2_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Unable to convert function return value to a Python type! The signature was\n\t() -> handle"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import chi2_contingency\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['LOKY_MAX_CPU_COUNT'] = '4'  # Set to the number of CPU cores you want to use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948cb308-612a-4da1-98c0-736da467816f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# load dataset 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc920bb-c4b4-4172-98d8-84eabe3cb852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data types as a dictionary\n",
    "dtypes = {\n",
    "    'action_taken': 'int32',\n",
    "    'loan_type': 'int32',\n",
    "    'lien_status': 'int32',\n",
    "    'open_end_line_of_credit': 'int32',\n",
    "    'loan_amount': 'int32',\n",
    "    'combined_loan_to_value_ratio': 'int32',\n",
    "    'interest_rate': 'float32',\n",
    "    'total_loan_costs': 'float32',\n",
    "    'origination_charges': 'float32',\n",
    "    'loan_term': 'float32',\n",
    "    'negative_amortization': 'int32',\n",
    "    'interest_only_payment': 'int32',\n",
    "    'balloon_payment': 'int32',\n",
    "    'other_nonamortizing_features': 'int32',\n",
    "    'property_value': 'float32',\n",
    "    'occupancy_type': 'int32',\n",
    "    'manufactured_home_secured_property_type': 'int32',\n",
    "    'manufactured_home_land_property_interest': 'int32',\n",
    "    'total_units': 'int32',\n",
    "    'income': 'float32',\n",
    "    'debt_to_income_ratio': 'float32',\n",
    "    'applicant_credit_score_type': 'int32',\n",
    "    'co_applicant_credit_score_type': 'int32',\n",
    "    'applicant_ethnicity_1': 'float32',\n",
    "    'co_applicant_ethnicity_1': 'float32',\n",
    "    'applicant_race_1': 'float32',\n",
    "    'applicant_race_2': 'float32',\n",
    "    'co_applicant_race_1': 'float32',\n",
    "    'co_applicant_race_2': 'float32',\n",
    "    'applicant_sex': 'int32',\n",
    "    'co_applicant_sex': 'int32',\n",
    "    'applicant_age': 'int32',\n",
    "    'co_applicant_age': 'int32',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2a59209-78cf-4057-853f-b81f2fd701a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the CSV file with specified data types\n",
    "df = pd.read_csv('10_all_numerical_32bit.csv', dtype=dtypes, low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac230a67-7192-4ee7-a1f6-bbe1fa3f771e",
   "metadata": {},
   "source": [
    "# Generate Sub-Datasets: \n",
    "For each sensitive attribute, create sub-datasets by grouping the data based on the different sensitive options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4a32da-23a9-4e28-8266-6a2074684532",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the dataset into \"males\" and \"females\" and \"joint\"\n",
    "males = df[df['applicant_sex'] == 1] \n",
    "females = df[df['applicant_sex'] == 2]  \n",
    "joint = df[df['applicant_sex'] == 6]  \n",
    "\n",
    "# Create sub-datasets for males and females\n",
    "sub_datasets = {\n",
    "    \"Males\": males,\n",
    "    \"Females\": females,\n",
    "    \"Joint\": joint\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5879b92-5b7f-4c1e-a5c5-16a2a8a4be92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['applicant_race_1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af0a2b9-f534-461d-9650-56c68a5a2021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Split the 'Males' dataset based on 'applicant_race_1'\n",
    "males_white = males[males['applicant_race_1'] == 5]\n",
    "males_black = males[males['applicant_race_1'] == 3]\n",
    "\n",
    "# Split the 'Females' dataset based on 'applicant_race_1'\n",
    "females_white = females[females['applicant_race_1'] == 5]\n",
    "females_black = females[females['applicant_race_1'] == 3]\n",
    "\n",
    "# Split the 'Joint' dataset based on 'applicant_race_1'\n",
    "joint_white = joint[joint['applicant_race_1'] == 5]\n",
    "joint_black = joint[joint['applicant_race_1'] == 3]\n",
    "\n",
    "# Create sub-datasets for the resulting combinations\n",
    "sub_datasets = {\n",
    "    \"Males White\": males_white,\n",
    "    \"Males Black\": males_black,\n",
    "    \"Females White\": females_white,\n",
    "    \"Females Black\": females_black,\n",
    "    \"Joint White\": joint_white,\n",
    "    \"Joint Black\": joint_black\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4f6cae-6d69-4531-ad2c-35ab737924b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the 'Males White' dataset based on 'applicant_ethnicity_1'\n",
    "males_white_latino = males_white[males_white['applicant_ethnicity_1'] == 1.000000]\n",
    "males_white_not_latino = males_white[males_white['applicant_ethnicity_1'] == 2.000000]\n",
    "\n",
    "# Split the 'Males Black' dataset based on 'applicant_ethnicity_1'\n",
    "males_black_latino = males_black[males_black['applicant_ethnicity_1'] == 1.000000]\n",
    "males_black_not_latino = males_black[males_black['applicant_ethnicity_1'] == 2.000000]\n",
    "\n",
    "# Split the 'Females White' dataset based on 'applicant_ethnicity_1'\n",
    "females_white_latino = females_white[females_white['applicant_ethnicity_1'] == 1.000000]\n",
    "females_white_not_latino = females_white[females_white['applicant_ethnicity_1'] == 2.000000]\n",
    "\n",
    "# Split the 'Females Black' dataset based on 'applicant_ethnicity_1'\n",
    "females_black_latino = females_black[females_black['applicant_ethnicity_1'] == 1.000000]\n",
    "females_black_not_latino = females_black[females_black['applicant_ethnicity_1'] == 2.000000]\n",
    "\n",
    "# Split the 'Joint White' dataset based on 'applicant_ethnicity_1'\n",
    "joint_white_latino = joint_white[joint_white['applicant_ethnicity_1'] == 1.000000]\n",
    "joint_white_not_latino = joint_white[joint_white['applicant_ethnicity_1'] == 2.000000]\n",
    "\n",
    "# Split the 'Joint Black' dataset based on 'applicant_ethnicity_1'\n",
    "joint_black_latino = joint_black[joint_black['applicant_ethnicity_1'] == 1.000000]\n",
    "joint_black_not_latino = joint_black[joint_black['applicant_ethnicity_1'] == 2.000000]\n",
    "\n",
    "# Create sub-datasets for the resulting combinations\n",
    "sub_datasets = {\n",
    "    \"Males White Latino\": males_white_latino,\n",
    "    \"Males White Not Latino\": males_white_not_latino,\n",
    "    \"Males Black Latino\": males_black_latino,\n",
    "    \"Males Black Not Latino\": males_black_not_latino,\n",
    "    \"Females White Latino\": females_white_latino,\n",
    "    \"Females White Not Latino\": females_white_not_latino,\n",
    "    \"Females Black Latino\": females_black_latino,\n",
    "    \"Females Black Not Latino\": females_black_not_latino,\n",
    "    \"Joint White Latino\": joint_white_latino,\n",
    "    \"Joint White Not Latino\": joint_white_not_latino,\n",
    "    \"Joint Black Latino\": joint_black_latino,\n",
    "    \"Joint Black Not Latino\": joint_black_not_latino\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af902abf-640b-4c2b-9120-d90b60d6bb6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a list to store the sizes of the subsets\n",
    "subset_sizes = []\n",
    "\n",
    "# Iterate through the sub-datasets in your dictionary and calculate the size\n",
    "for sub_dataset_name, sub_dataset in sub_datasets.items():\n",
    "    subset_size = len(sub_dataset)\n",
    "    subset_sizes.append({'Subset Name': sub_dataset_name, 'Size': subset_size})\n",
    "\n",
    "# Create a DataFrame from the list of subset sizes\n",
    "subset_sizes_df = pd.DataFrame(subset_sizes)\n",
    "\n",
    "# Print the subset sizes table\n",
    "subset_sizes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132b8448-8c2c-478b-8842-e9fbd6f0bbc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0875a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
